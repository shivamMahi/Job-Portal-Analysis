# Job-Portal-Analysis
The Datasets are taken from Kaggle and consists of 22000 rows and 7-8 column each.

The dataset is split into various subset for multiple uses.

The First Part of Analysis is storing the Raw Data in Mysql Database.

The Second Part Contains importing data to Pyspark using Jupyter notebook , converting them to PySpark Dataframes and perform cleaning operations like Data Cleaning, Feature Engineering Part i.e complete EDA (Exploratory Data Analysis).

Feature Engineering is divided into : -
 Feature Scaling 
 Feature Transformation  
 Feature Addition/Constructions

The Third Part is storing the structured data into Mysql Database for furture Analysis and storing data locally in Parquet Format.

The Forth Part is creating Pyspark Optimized Jobs to get various insights from data and converted some jobs to Pandas to show results visually using Pandas Seaborn and Matplotlib libraries.

The Fifth Part is fetching data from MySql Database to Tableau 10 for showing data Visually in a Story Manner using Tooltip, Dashboards and Stories.

The Output File of Analysis is .ipynb and .md files and files could be seen on any jupter platform after downloading it.
